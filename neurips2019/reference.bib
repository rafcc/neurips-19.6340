@article{Borges2002,
 title = "Total least squares fitting of {B\'ezier} and {B-spline} curves to ordered data",
 journal = "Computer Aided Geometric Design",
 volume = "19",
 number = "4",
 pages = "275--289",
 year = "2002",
 issn = "0167-8396",
 doi = "https://doi.org/10.1016/S0167-8396(02)00088-2",
 url = "http://www.sciencedirect.com/science/article/pii/S0167839602000882",
 author = "Carlos F. Borges and Tim Pastva",
 keywords = "Data fitting, Total least-squares, B-splines"
}

@inproceedings{Hamada2010,
 author = {Hamada, Naoki and Nagata, Yuichi and Kobayashi, Shigenobu and Ono, Isao},
 title = {Adaptive Weighted Aggregation: A Multiobjective Function Optimization Framework Taking Account of Spread and Evenness of Approximate Solutions},
 booktitle = {Proceedings of the 2010 IEEE Congress on Evolutionary Computation},
 series = {CEC 2010},
 year = {2010},
 pages = {787--794},
}

@inproceedings{Hamada2017,
 author = {Hamada, Naoki},
 title = {Simple Problems: The Simplicial Gluing Structure of {Pareto} Sets and {Pareto} Fronts},
 booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
 series = {GECCO '17},
 year = {2017},
 isbn = {978-1-4503-4939-0},
 location = {Berlin, Germany},
 pages = {315--316},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/3067695.3076069},
 doi = {10.1145/3067695.3076069},
 acmid = {3076069},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {multi-objective optimization, problem class, stratification},
}

@article{Hamada2019,
  author = {Hamada, Naoki and Hayano, Kenta and Ichiki, Shunsuke and Kabata, Yutaro and Teramoto, Hiroshi},
  title = {Topology of {Pareto} Sets of Strongly Convex Problems},
  journal = {ArXiv e-prints},
  archivePrefix = "arXiv",
  eprint = {1904.03615},
  primaryClass = "math.OC",
  keywords = {Mathematics - Optimization and Control, 90C26 - 90C29 - 58K05},
  year = 2019,
  note = {\url{http://arxiv.org/abs/1904.03615}}
}

@inproceedings{Harada2007,
 author = {Harada, K. and Sakuma, J. and Kobayashi, S. and Ono, I.},
 title = {Uniform Sampling of Local {Pareto}-Optimal Solution Curves by {Pareto} Path Following and its Applications in Multi-objective {GA}},
 booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference (GECCO)},
 year = {2007},
 isbn = {978-1-59593-697-4},
 location = {London, England},
 pages = {813--820},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1276958.1277120},
 doi = {http://doi.acm.org/10.1145/1276958.1277120},
 acmid = {1277120},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {constraint handling, local search, multi-objective optimization},
}

@book{Miettinen1999,
  title={Nonlinear Multiobjective Optimization},
  author={Miettinen, Kaisa M.},
  isbn={9780792382782},
  lccn={98037888},
  series={International Series in Operations Research \& Management Science},
  volume = {12},
  year={1999},
  publisher={Springer-Verlag, GmbH}
}

@book{Eichfelder2008,
 author = {Eichfelder, G.},
 title = {Adaptive Scalarization Methods in Multiobjective Optimization},
 year = {2008},
 publisher = {Springer-Verlag, Berlin, Heidelberg},
 isbn = {3540791574},
}

@article{Zhang2007,
 author={Zhang, Q. and Li, H.},
 title={{MOEA/D}: A Multiobjective Evolutionary Algorithm Based on Decomposition},
 journal={IEEE Transactions on Evolutionary Computation},
 year={2007},
 volume={11},
 number={6},
 pages={712--731},
 keywords={computational complexity, genetic algorithmscomputational complexity, decomposition, genetic algorithm, knapsack problem, multiobjective evolutionary algorithm, scalar optimization subproblem},
 doi={10.1109/TEVC.2007.892759},
 ISSN={1089-778X},
}

@ARTICLE{Deb2014,
author={Deb, K. and Jain, H.},
journal={IEEE Transactions on Evolutionary Computation},
title={An Evolutionary Many-Objective Optimization Algorithm Using Reference-Point-Based Nondominated Sorting Approach, Part {I}: Solving Problems With Box Constraints},
year={2014},
volume={18},
number={4},
pages={577-601},
keywords={genetic algorithms;sorting;EMO algorithms;MOEA/D methods;NSGA-II framework;NSGA-III;box constraints;evolutionary many-objective optimization algorithm;evolutionary multiobjective optimization algorithms;many-objective optimization problems;many-objective test problems;reference points;reference-point-based many-objective evolutionary algorithm;reference-point-based nondominated sorting approach;unconstrained problems;Educational institutions;Measurement;Optimization;Sociology;Statistics;Vectors;Zirconium;Evolutionary computation;Many-objective optimization;NSGA-III;evolutionary computation;large dimension;many-objective optimization;multi-criterion optimization;multicriterion optimization;non-dominated sorting;nondominated sorting},
doi={10.1109/TEVC.2013.2281535},
ISSN={1089-778X},
}

@InProceedings{Hernandez-Lobato2016,
  title = 	 {Predictive Entropy Search for Multi-objective Bayesian Optimization},
  author = 	 {Daniel Hernandez-Lobato and Jose Hernandez-Lobato and Amar Shah and Ryan Adams},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {1492--1501},
  year = 	 {2016},
  editor = 	 {Maria Florina Balcan and Kilian Q. Weinberger},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/hernandez-lobatoa16.pdf},
  url = 	 {http://proceedings.mlr.press/v48/hernandez-lobatoa16.html},
  abstract = 	 {We present \small PESMO, a Bayesian method for identifying the Pareto set of multi-objective optimization problems, when the functions are expensive to evaluate. \small PESMO chooses the evaluation points to maximally reduce the entropy of the posterior distribution over the Pareto set. The \small PESMO acquisition function is decomposed as a sum of objective-specific acquisition functions, which makes it possible to use the algorithm in \emphdecoupled scenarios in which the objectives can be evaluated separately and perhaps with different costs. This decoupling capability is useful to identify difficult objectives that require more evaluations. \small PESMO also offers gains in efficiency, as its cost scales linearly with the number of objectives, in comparison to the exponential cost of other methods. We compare \small PESMO with other methods on synthetic and real-world problems. The results show that \small PESMO produces better recommendations with a smaller number of evaluations, and that a decoupled evaluation can lead to improvements in performance, particularly when the number of objectives is large.}
}

@article{Yang2019,
title = {Multi-Objective {Bayesian} Global Optimization using expected hypervolume improvement gradient},
journal = "Swarm and Evolutionary Computation",
volume = "44",
pages = "945--956",
year = "2019",
issn = "2210-6502",
doi = "https://doi.org/10.1016/j.swevo.2018.10.007",
url = "http://www.sciencedirect.com/science/article/pii/S2210650217307861",
author = {Kaifeng Yang and Michael Emmerich and Andr\'e Deutz and Thomas B\"ack},
keywords = "Bayesian global optimization, Expected hypervolume improvement, Expected hypervolume improvement gradient, Kriging stopping criterion",
abstract = "The Expected Hypervolume Improvement (EHVI) is a frequently used infill criterion in Multi-Objective Bayesian Global Optimization (MOBGO), due to its good ability to lead the exploration. Recently, the computational complexity of EHVI calculation is reduced to O(n log n) for both 2-D and 3-D cases. However, the optimizer in MOBGO still requires a significant amount of time, because the calculation of EHVI is carried out in each iteration and usually tens of thousands of the EHVI calculations are required. This paper derives a formula for the Expected Hypervolume Improvement Gradient (EHVIG) and proposes an efficient algorithm to calculate EHVIG. The new criterion (EHVIG) is utilized by two different strategies to improve the efficiency of the optimizer discussed in this paper. Firstly, it enables gradient ascent methods to be used in MOBGO. Moreover, since the EHVIG of an optimal solution should be a zero vector, it can be regarded as a stopping criterion in global optimization, e.g., in Evolution Strategies. Empirical experiments are performed on seven benchmark problems. The experimental results show that the second proposed strategy, using EHVIG as a stopping criterion for local search, can outperform the normal MOBGO on problems where the optimal solutions are located in the interior of the search space. For the ZDT series test problems, EHVIG still can perform better when gradient projection is applied."
}

@article {Shoval2012,
 author = {Shoval, O. and Sheftel, H. and Shinar, G. and Hart, Y. and Ramote, O. and Mayo, A. and Dekel, E. and Kavanagh, K. and Alon, U.},
 title = {Evolutionary Trade-Offs, {Pareto} Optimality, and the Geometry of Phenotype Space},
 volume = {336},
 number = {6085},
 pages = {1157--1160},
 year = {2012},
 doi = {10.1126/science.1217405},
 publisher = {American Association for the Advancement of Science},
 abstract = {Biological systems that perform multiple tasks face a fundamental trade-off: A given phenotype cannot be optimal at all tasks. Here we ask how trade-offs affect the range of phenotypes found in nature. Using the Pareto front concept from economics and engineering, we find that best{\textendash}trade-off phenotypes are weighted averages of archetypes{\textemdash}phenotypes specialized for single tasks. For two tasks, phenotypes fall on the line connecting the two archetypes, which could explain linear trait correlations, allometric relationships, as well as bacterial gene-expression patterns. For three tasks, phenotypes fall within a triangle in phenotype space, whose vertices are the archetypes, as evident in morphological studies, including on Darwin{\textquoteright}s finches. Tasks can be inferred from measured phenotypes based on the behavior of organisms nearest the archetypes.},
 issn = {0036-8075},
 URL = {http://science.sciencemag.org/content/336/6085/1157},
 eprint = {http://science.sciencemag.org/content/336/6085/1157.full.pdf},
 journal = {Science}
}

@article{Mastroddi2013,
 title = "Analysis of {Pareto} frontiers for multidisciplinary design optimization of aircraft",
 journal = "Aerosp. Sci. Technol.",
 volume = "28",
 number = "1",
 pages = "40--55",
 year = "2013",
 issn = "1270-9638",
 author = "F. Mastroddi and S. Gemma",
}

@article{Vrugt2003,
 author = {Vrugt, Jasper A. and Gupta, Hoshin V. and Bastidas, Luis A. and Bouten, Willem and  Sorooshian, Soroosh},
 title = {Effective and Efficient Algorithm for Multiobjective Optimization of Hydrologic Models},
 year = {2003},
 journal = {Water Resources Research},
 volume = {39},
 number = {8},
 pages = {1214--1232},
 publisher = {American Geophysical Union},
 issn = {0043-1397},
 keyword = {Stochastic processes, Hydrologic budget, Instruments and techniques},
 url = {http://dx.doi.org/10.1029/2002WR001746},
 doi = {10.1029/2002WR001746},
}

@article{Kuhn1967,
  title={On a pair of dual nonlinear programs},
  author={Kuhn, Harold W},
  journal={Nonlinear Programming},
  volume={1},
  pages={38--45},
  year={1967},
  publisher={North-Holland Amsterdam}
}

@article{Wan1975,
 title = "On Local {Pareto} Optima",
 journal = "Journal of Mathematical Economics",
 volume = "2",
 number = "1",
 pages = "35--42",
 year = "1975",
 issn = "0304-4068",
 doi = "http://dx.doi.org/10.1016/0304-4068(75)90012-9",
 url = "http://www.sciencedirect.com/science/article/pii/0304406875900129",
 author = "Yieh-Hei Wan"
}

@book{Deb2001,
 author = {Deb, K.},
 title = {Multi-objective Optimization Using Evolutionary Algorithms},
 year = {2001},
 isbn = {047187339X},
 publisher = {John Wiley \& Sons, Inc.},
 address = {New York, NY, USA},
}

@book{Hillermeier2001,
  author = {Hillermeier, C.},
  title = {Nonlinear Multiobjective Optimization: A Generalized Homotopy Approach},
  series = {International Series of Numerical Mathematics},
  volume = {25},
  publisher = {Birkh\"{a}user Verlag, Basel, Boston, Berlin},
  year = {2001},
}

@Article{Gebken2019,
 author="Gebken, Bennet
 and Peitz, Sebastian
 and Dellnitz, Michael",
 title="On the hierarchical structure of Pareto critical sets",
 journal="Journal of Global Optimization",
 year="2019",
 volume="73",
 number="4",
 pages="891--913",
 abstract="In this article we show that the boundary of the Pareto critical set of an unconstrained multiobjective optimization problem (MOP) consists of Pareto critical points of subproblems where only a subset of the set of objective functions is taken into account. If the Pareto critical set is completely described by its boundary (e.g., if we have more objective functions than dimensions in decision space), then this can be used to efficiently solve the MOP by solving a number of MOPs with fewer objective functions. If this is not the case, the results can still give insight into the structure of the Pareto critical set.",
 issn="1573-2916",
 doi="10.1007/s10898-019-00737-6",
 url="https://doi.org/10.1007/s10898-019-00737-6"
}

@misc{Mather1970,
 title={Notes on Topological Stability},
 author={Mather, J.},
 note={Lecture Notes},
 publisher={Harvard University},
 year={1970}
}

@inproceedings{Mather1971,
 title={Stratifications and Mappings},
 author={Mather, J.},
 booktitle={Proceedings of the Dynamical Systems Conference},
 pages={195-232},
 editor={M. Peixoto},
 publisher={Academic Press},
 address={Salvador, Brazil},
 year={1971}
}

@incollection{Mather1976,
 title={How to Stratify Mappings and Jet Spaces},
 author={Mather, J.},
 booktitle={Singularites d'Applications Differentiables S\'{e}m},
 address={Plans-sur-Bex},
 series={Lecture Notes in Mathematics},
 volume={535},
 pages={128--176},
 publisher={Springer},
 year={1976}
}

@inproceedings{Kobayashi2019,
  author = {Kobayashi, Ken and Hamada, Naoki and Sannai, Akiyoshi and Tanaka, Akinori and Bannai, Kenichi and Sugiyama, Masashi},
  title = {B\'ezier Simplex Fitting: Describing {Pareto} Fronts of Simplicial Problems with Small Samples in Multi-objective Optimization},
  booktitle = {Proceedings of the Thirty-Third {AAAI} Conference on Artificial Intelligence},
  series = {AAAI-19},
  eprint = {1812.05222},
  eprintclass = {math.OC},
  year = {to appear},
}

@inproceedings{Harada2006,
 author = {Harada, Ken and Sakuma, Jun and Kobayashi, Shigenobu},
 title = {Local Search for Multiobjective Function Optimization: {Pareto} Descent Method},
 booktitle = {Proceedings of the 8th Annual Conference on Genetic and Evolutionary Computation},
 series = {GECCO '06},
 year = {2006},
 isbn = {1-59593-186-4},
 location = {Seattle, Washington, USA},
 pages = {659--666},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1143997.1144115},
 doi = {10.1145/1143997.1144115},
 acmid = {1144115},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {constraint handling, local search, multi-objective optimization},
}

@book{Hosmer1989,
 author = {David W Hosmer and Stanley Lemeshow},
 title = {Applied Logistic Regression},
 year  = {1989},
 publisher = {Wiley},
 address = {New York}
}

@book{Venables2002,
 author = {Venables, W. N. and Ripley, B. D.},
 title = {Modern Applied Statistics with {S}},
 edition = {Fourth},
 year = {2002},
 publisher = {Springer},
} 

@article{Hoerl1970,
 author = { Arthur E.   Hoerl  and  Robert W.   Kennard },
 title = {Ridge Regression: Biased Estimation for Nonorthogonal Problems},
 journal = {Technometrics},
 volume = {12},
 number = {1},
 pages = {55-67},
 year  = {1970},
 publisher = {Taylor \& Francis},
 doi = {10.1080/00401706.1970.10488634},
 URL = {https://www.tandfonline.com/doi/abs/10.1080/00401706.1970.10488634},
 eprint = {https://www.tandfonline.com/doi/pdf/10.1080/00401706.1970.10488634}
}

@article{Frank1993,
 author = {lldiko E. Frank and Jerome H. Friedman},
 title = {A Statistical View of Some Chemometrics Regression Tools},
 journal = {Technometrics},
 volume = {35},
 number = {2},
 pages = {109--135},
 year  = {1993},
 publisher = {Taylor \& Francis},
 doi = {10.1080/00401706.1993.10485033},
 URL = {https://www.tandfonline.com/doi/abs/10.1080/00401706.1993.10485033},
 eprint = {https://www.tandfonline.com/doi/pdf/10.1080/00401706.1993.10485033}
}

@article{Tibshirani1996,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2346178},
 abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
 author = {Robert Tibshirani},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
 number = {1},
 pages = {267--288},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Regression Shrinkage and Selection via the Lasso},
 volume = {58},
 year = {1996}
}

@article{Tibshirani2005,
 author = {Tibshirani, Robert and Saunders, Michael and Rosset, Saharon and Zhu, Ji and Knight, Keith},
 title = {Sparsity and smoothness via the fused lasso},
 journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
 volume = {67},
 number = {1},
 pages = {91--108},
 keywords = {Fused lasso, Gene expression, Lasso, Least squares regression, Protein mass spectroscopy, Sparse solutions, Support vector classifier},
 doi = {10.1111/j.1467-9868.2005.00490.x},
 url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2005.00490.x},
 eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9868.2005.00490.x},
 abstract = {Summary.? The lasso penalizes a least squares regression by the sum of the absolute values (L1-norm) of the coefficients. The form of this penalty encourages sparse solutions (with many coefficients equal to 0). We propose the ‘fused lasso’, a generalization that is designed for problems with features that can be ordered in some meaningful way. The fused lasso penalizes the L1-norm of both the coefficients and their successive differences. Thus it encourages sparsity of the coefficients and also sparsity of their differences?i.e. local constancy of the coefficient profile. The fused lasso is especially useful when the number of features p is much greater than N, the sample size. The technique is also extended to the ‘hinge’ loss function that underlies the support vector classifier. We illustrate the methods on examples from protein mass spectroscopy and gene expression data.},
year = {2005}
}

@article{Zou2005,
 author = {Hui Zou and Trevor Hastie},
 journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
 number = {2},
 pages = {301--320},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Regularization and Variable Selection via the Elastic Net},
 volume = {67},
 ISSN = {13697412, 14679868},
 URL = {http://www.jstor.org/stable/3647580},
 year = {2005}
}

@article{Yuan2006,
 author = {Yuan, Ming and Lin, Yi},
 title = {Model selection and estimation in regression with grouped variables},
 journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
 volume = {68},
 number = {1},
 pages = {49-67},
 keywords = {Analysis of variance, Lasso, Least angle regression, Non-negative garrotte, Piecewise linear solution path},
 doi = {10.1111/j.1467-9868.2005.00532.x},
 url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2005.00532.x},
 eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9868.2005.00532.x},
 abstract = {Summary.? We consider the problem of selecting grouped variables (factors) for accurate prediction in regression. Such a problem arises naturally in many practical situations with the multifactor analysis-of-variance problem as the most important and well-known example. Instead of selecting factors by stepwise backward elimination, we focus on the accuracy of estimation and consider extensions of the lasso, the LARS algorithm and the non-negative garrotte for factor selection. The lasso, the LARS algorithm and the non-negative garrotte are recently proposed regression methods that can be used to select individual variables. We study and propose efficient algorithms for the extensions of these methods for factor selection and show that these extensions give superior performance to the traditional stepwise backward elimination method in factor selection problems. We study the similarities and the differences between these methods. Simulations and real examples are used to illustrate the methods.},
 year = {2006}
}

@article{Hebiri2011,
 author = "Hebiri, Mohamed and van de Geer, Sara",
 doi = "10.1214/11-EJS638",
 fjournal = "Electronic Journal of Statistics",
 journal = "Electron. J. Statist.",
 pages = "1184--1226",
 publisher = "The Institute of Mathematical Statistics and the Bernoulli Society",
 title = "The Smooth-Lasso and other $\ell_1 + \ell_2$-penalized methods",
 url = "https://doi.org/10.1214/11-EJS638",
 volume = "5",
 year = "2011"
}
